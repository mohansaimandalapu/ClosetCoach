{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ClosetCoach Welcome to ClosetCoach, a Fashion Wardrobe Assistant project, designed to help you develop your Deep Learning based Computer Vision skills. In this project, we will guide you through the process of building a fashion wardrobe assistant from scratch, using cutting-edge Deep Learning techniques. ClosetCoach Features Product Attribute Extraction Digital wardrobe creation User Style Creation Fashion Recommendation based on User Style Project Structure Project Roadmap Contribution guidelines ClosetCoach Features We've designed ClosetCoach with features that cater to your every fashion need. Here's a closer look at what we offer: Product Attribute Extraction Have you ever wondered what category your clothes fall under? Our advanced algorithm can analyze your attire picture and tag it with all the product and meta attributes, so you don't have to. Our feature can recognize product super categories like TopWear, BottomWear, FootWear, Sports Wear, Accessories, SleepWear, and Ethnic and FestiveWear. We also recognize specific product categories like T-shirts, Shirts, Jeans, Casual Trousers, Formal Trousers, Shorts, and Track Pants & many more.Additionally, we provide product metadata, including color information, product style, print type, and much more, so you have a complete understanding of your clothes. Digital Wardrobe Creation No more digging through piles of clothes in your closet! Our digital wardrobe creation feature enables you to upload pictures of your clothes and create a virtual wardrobe. Now you can access all your clothes in one place and easily plan your outfits for the week. User Style Creation Defining your unique style can be challenging, but not with ClosetCoach. Based on your digital wardrobe, our algorithm will analyze your clothes and create a personalized style profile that captures your unique style. Our feature will provide you with fashion recommendations that align with your style preferences, ensuring you look your best every time you step out. Fashion Recommendation based on User Style Ready to explore new fashion trends? Our fashion recommendation feature is here to help you discover new clothes that fit your personal style. Whether you want to find clothes that go well with your existing clothes in your digital wardrobe or explore new attire that aligns with your style preferences, we've got you covered. With ClosetCoach, you can discover new fashion pieces effortlessly. We understand that fashion is a personal journey, and we're here to help you every step of the way. Our features are designed to make fashion accessible, enjoyable, and effortless. So why wait? Sign up today and start exploring your style with ClosetCoach! Project Structure Firstly, let me explain that this project is split into two main parts -Codebase - Documentation plus Tutorials. The Codebase is where you'll find all the actual project files, including the source code, which is located in the ClosetCoach folder under the project root directory. This is where the real magic happens, where all the coding and development work takes place to bring the project to life. The second part of the project is the Documentation plus Tutorials. This is where we'll be providing helpful guides, tips and tutorials on how a particular feature of the project was developed and how to use it . Every alternate week, we'll be updating this section with new information on how specific features were implemented, as well as tutorials on how to use them. You can find this section in the docs folder located in the project root directory. We believe that having a strong Documentation plus Tutorials section is just as important as having a well-functioning Codebase. This is because it's the place where users can learn how to implement features of ClosetCoach using Deep learning based Computer Vision skills and find helpful information on how to use the project to its full potential. We understand that sometimes technical jargon can be intimidating, which is why we're committed to making our documentation as user-friendly and engaging as possible. Project Roadmap Are you curious about what features ClosetCoach has in store for you in the coming weeks and months? Look no further than our roadmap! Our roadmap provides you with an overview of the features we are currently working on, what stage they are in, and when we expect to release them. We welcome feedback from our users through our public discussion forums , as we believe in the power of collaboration and the open source community. Our project board is organized by priority and timeline, making it easy for you to follow our progress and stay up-to-date on the latest developments.We are committed to providing the best possible experience for our users and believe that the open source nature of our project will allow for continued growth and improvement. Join us on our journey to improve computer vision skills and create a thriving open source community! Contribution guidelines Issues We use Github issues to track public bugs and feature updates or enhancements and public discussion forums for any questions or feedback issues. Please make sure to follow one of the issue templates when reporting any issues. Pull Requests We actively welcome pull requests. However, if you're adding any significant features (e.g. > 50 lines), please make sure to discuss with maintainers about your motivation and proposals in an issue before sending a PR. This is to save your time so you don't spend time on a PR that we'll not accept.","title":"ClosetCoach"},{"location":"#closetcoach","text":"Welcome to ClosetCoach, a Fashion Wardrobe Assistant project, designed to help you develop your Deep Learning based Computer Vision skills. In this project, we will guide you through the process of building a fashion wardrobe assistant from scratch, using cutting-edge Deep Learning techniques. ClosetCoach Features Product Attribute Extraction Digital wardrobe creation User Style Creation Fashion Recommendation based on User Style Project Structure Project Roadmap Contribution guidelines","title":"ClosetCoach"},{"location":"#closetcoach-features","text":"We've designed ClosetCoach with features that cater to your every fashion need. Here's a closer look at what we offer:","title":"ClosetCoach Features"},{"location":"#product-attribute-extraction","text":"Have you ever wondered what category your clothes fall under? Our advanced algorithm can analyze your attire picture and tag it with all the product and meta attributes, so you don't have to. Our feature can recognize product super categories like TopWear, BottomWear, FootWear, Sports Wear, Accessories, SleepWear, and Ethnic and FestiveWear. We also recognize specific product categories like T-shirts, Shirts, Jeans, Casual Trousers, Formal Trousers, Shorts, and Track Pants & many more.Additionally, we provide product metadata, including color information, product style, print type, and much more, so you have a complete understanding of your clothes.","title":"Product Attribute Extraction"},{"location":"#digital-wardrobe-creation","text":"No more digging through piles of clothes in your closet! Our digital wardrobe creation feature enables you to upload pictures of your clothes and create a virtual wardrobe. Now you can access all your clothes in one place and easily plan your outfits for the week.","title":"Digital Wardrobe Creation"},{"location":"#user-style-creation","text":"Defining your unique style can be challenging, but not with ClosetCoach. Based on your digital wardrobe, our algorithm will analyze your clothes and create a personalized style profile that captures your unique style. Our feature will provide you with fashion recommendations that align with your style preferences, ensuring you look your best every time you step out.","title":"User Style Creation"},{"location":"#fashion-recommendation-based-on-user-style","text":"Ready to explore new fashion trends? Our fashion recommendation feature is here to help you discover new clothes that fit your personal style. Whether you want to find clothes that go well with your existing clothes in your digital wardrobe or explore new attire that aligns with your style preferences, we've got you covered. With ClosetCoach, you can discover new fashion pieces effortlessly. We understand that fashion is a personal journey, and we're here to help you every step of the way. Our features are designed to make fashion accessible, enjoyable, and effortless. So why wait? Sign up today and start exploring your style with ClosetCoach!","title":"Fashion Recommendation based on User Style"},{"location":"#project-structure","text":"Firstly, let me explain that this project is split into two main parts -Codebase - Documentation plus Tutorials. The Codebase is where you'll find all the actual project files, including the source code, which is located in the ClosetCoach folder under the project root directory. This is where the real magic happens, where all the coding and development work takes place to bring the project to life. The second part of the project is the Documentation plus Tutorials. This is where we'll be providing helpful guides, tips and tutorials on how a particular feature of the project was developed and how to use it . Every alternate week, we'll be updating this section with new information on how specific features were implemented, as well as tutorials on how to use them. You can find this section in the docs folder located in the project root directory. We believe that having a strong Documentation plus Tutorials section is just as important as having a well-functioning Codebase. This is because it's the place where users can learn how to implement features of ClosetCoach using Deep learning based Computer Vision skills and find helpful information on how to use the project to its full potential. We understand that sometimes technical jargon can be intimidating, which is why we're committed to making our documentation as user-friendly and engaging as possible.","title":"Project Structure"},{"location":"#project-roadmap","text":"Are you curious about what features ClosetCoach has in store for you in the coming weeks and months? Look no further than our roadmap! Our roadmap provides you with an overview of the features we are currently working on, what stage they are in, and when we expect to release them. We welcome feedback from our users through our public discussion forums , as we believe in the power of collaboration and the open source community. Our project board is organized by priority and timeline, making it easy for you to follow our progress and stay up-to-date on the latest developments.We are committed to providing the best possible experience for our users and believe that the open source nature of our project will allow for continued growth and improvement. Join us on our journey to improve computer vision skills and create a thriving open source community!","title":"Project Roadmap"},{"location":"#contribution-guidelines","text":"","title":"Contribution guidelines"},{"location":"#issues","text":"We use Github issues to track public bugs and feature updates or enhancements and public discussion forums for any questions or feedback issues. Please make sure to follow one of the issue templates when reporting any issues.","title":"Issues"},{"location":"#pull-requests","text":"We actively welcome pull requests. However, if you're adding any significant features (e.g. > 50 lines), please make sure to discuss with maintainers about your motivation and proposals in an issue before sending a PR. This is to save your time so you don't spend time on a PR that we'll not accept.","title":"Pull Requests"},{"location":"setup/","text":"Welcome to ClosetCoach - Project Setup Guide Thank you for your interest in the ClosetCoach project! This guide will walk you through setting up the project on your local machine, so you can use it, contribute to it, or just explore the code. Prerequisites Before you dive into the setup process, ensure that you have the following software installed on your computer: Python 3.8 or later: Download Python Git: Download Git Poetry: Install Poetry Step 1: Clone the Repository Begin by cloning the repository to your local machine. Run the following command in your terminal or command prompt: git clone https://github.com/satishjasthi/ClosetCoach.git Next, navigate to the project directory: cd ClosetCoach Step 2: Set up a Virtual Environment To isolate project dependencies, we recommend using a virtual environment. Please note that whenever the term 'python' is used in this guide, it refers to your locally installed Python 3.8 or later version. Create a virtual environment using the command below: python -m venv venv Activate the virtual environment: For Windows: powershellCopy code venv\\Scripts\\activate For macOS/Linux: source venv/bin/activate Step 3: Install Dependencies with Poetry Now it's time to install the required dependencies using Poetry: poetry install Step 4: Scraping Data With the dependencies installed, you're ready to start scraping the necessary data for the project. There are two spiders in the closetcoach_crawler directory. The first one scrapes product page URLs from various product categories, while the second one extracts product data from the collected product page URLs on Myntra. To run the product page URL scraper, execute the following command: cd closetcoach_crawler python crawl_urls.py product_page_url_scraper Once you have scraped the product page URLs, you can proceed to run the product data scraper with this command: cd closetcoach_crawler python crawl_urls.py product_details_scraper That's it! You've successfully set up the ClosetCoach project on your local machine. Feel free to explore the code, or contribute to the project. If you encounter any issues or have questions, don't hesitate to create an issue on the GitHub repository. We appreciate your support and are excited to have you on board!","title":"Setup"},{"location":"setup/#welcome-to-closetcoach-project-setup-guide","text":"Thank you for your interest in the ClosetCoach project! This guide will walk you through setting up the project on your local machine, so you can use it, contribute to it, or just explore the code.","title":"Welcome to ClosetCoach - Project Setup Guide"},{"location":"setup/#prerequisites","text":"Before you dive into the setup process, ensure that you have the following software installed on your computer: Python 3.8 or later: Download Python Git: Download Git Poetry: Install Poetry","title":"Prerequisites"},{"location":"setup/#step-1-clone-the-repository","text":"Begin by cloning the repository to your local machine. Run the following command in your terminal or command prompt: git clone https://github.com/satishjasthi/ClosetCoach.git Next, navigate to the project directory: cd ClosetCoach","title":"Step 1: Clone the Repository"},{"location":"setup/#step-2-set-up-a-virtual-environment","text":"To isolate project dependencies, we recommend using a virtual environment. Please note that whenever the term 'python' is used in this guide, it refers to your locally installed Python 3.8 or later version. Create a virtual environment using the command below: python -m venv venv Activate the virtual environment: For Windows: powershellCopy code venv\\Scripts\\activate For macOS/Linux: source venv/bin/activate","title":"Step 2: Set up a Virtual Environment"},{"location":"setup/#step-3-install-dependencies-with-poetry","text":"Now it's time to install the required dependencies using Poetry: poetry install","title":"Step 3: Install Dependencies with Poetry"},{"location":"setup/#step-4-scraping-data","text":"With the dependencies installed, you're ready to start scraping the necessary data for the project. There are two spiders in the closetcoach_crawler directory. The first one scrapes product page URLs from various product categories, while the second one extracts product data from the collected product page URLs on Myntra. To run the product page URL scraper, execute the following command: cd closetcoach_crawler python crawl_urls.py product_page_url_scraper Once you have scraped the product page URLs, you can proceed to run the product data scraper with this command: cd closetcoach_crawler python crawl_urls.py product_details_scraper That's it! You've successfully set up the ClosetCoach project on your local machine. Feel free to explore the code, or contribute to the project. If you encounter any issues or have questions, don't hesitate to create an issue on the GitHub repository. We appreciate your support and are excited to have you on board!","title":"Step 4: Scraping Data"},{"location":"tutorials/intro_to_scrapy/","text":"Introduction to Scrapy In this tutorial, we will learn about Scrapy spiders and how they can be used for web scraping. We will cover the following topics: Introduction to Scrapy spiders Creating a simple spider Setting up a Scrapy project Using Scrapy.Item Configuring Scrapy middleware Creating a basic Scrapy pipeline Introduction to Scrapy Spiders Scrapy spiders are classes that define how a specific site or group of sites should be scraped, including how to perform the crawl (i.e., follow links) and how to extract structured data from their pages (i.e., scraping items). Spiders are the main building blocks of a Scrapy project and are responsible for managing the process of gathering data from websites. What are Scrapy spiders used for? Scrapy spiders are used to: Crawl web pages and follow links to other pages. Extract and process structured data from web pages. Store the extracted data in the desired format, such as JSON, CSV, or XML. Scrapy Spider scrapy.Spider is the base class for all Scrapy spiders. To create a simple spider, you need to subclass scrapy.Spider and define the following attributes and methods: name : A unique identifier for the spider. start_urls : A list of URLs where the spider will begin to crawl. parse() : A method that will be called to handle the response downloaded for each of the requests made. Here's an example of a simple spider: import scrapy class ExampleSpider ( scrapy . Spider ): name = \"example_spider\" start_urls = [ 'http://example.com' ] def parse ( self , response ): self . log ( 'Visited %s ' % response . url ) Scrapy Project Setup Overview To set up a new Scrapy project, run the following command in your terminal: scrapy startproject myproject This will create a new directory named myproject with the following structure: myproject/ scrapy.cfg myproject/ __init__.py items.py middlewares.py pipelines.py settings.py spiders/ __init__.py The Scrapy project structure is a standardized layout for organizing your Scrapy spider code. The structure consists of a top-level project directory, which contains a configuration file and a subdirectory with the same name as the project. This subdirectory is where the majority of the code resides. Here's a breakdown of the files and directories in the Scrapy project structure: scrapy.cfg : This is the main configuration file for your Scrapy project. It tells Scrapy how to find your project code and where to store output files. myproject/ : This is the subdirectory with the same name as your project. It contains the bulk of your Scrapy code. init.py : This is an empty file that tells Python to treat the directory as a Python package. items.py : This file defines the data items that your spider will extract from websites. Each item corresponds to a specific piece of information that you want to scrape. middlewares.py: This file contains code for manipulating requests and responses as they pass through the Scrapy framework. This is useful for things like modifying headers or handling cookies. pipelines.py : This file defines the processing steps that your scraped items will go through. This can include things like cleaning and validating data or storing it in a database. settings.py: This file contains all of the settings for your Scrapy project, including things like user agents, download delays, and database connection information. spiders/ : This subdirectory is where you'll put the actual spider code. Each spider is defined in its own Python file in this directory. init.py : This is another empty file that tells Python to treat the directory as a Python package. Overall, this structure helps keep your Scrapy project organized and easy to navigate. By separating your code into logical modules, you can more easily maintain and extend your spiders as your scraping needs evolve. What is Scrapy.Item and How to Create One Scrapy items are simple Python classes used to define the structure of the data you want to scrape. They work like containers and allow you to define custom fields for the data you want to collect. Here's an example of how to create a Scrapy item: import scrapy class ExampleItem ( scrapy . Item ): title = scrapy . Field () url = scrapy . Field () description = scrapy . Field () What is Scrapy Middleware and How to Configure It Scrapy middleware is a way to process the requests and responses that flow through the Scrapy engine. Middleware components can be used to modify the requests and responses or to perform additional processing on them. To configure a middleware in Scrapy, add it to the MIDDLEWARES setting in your project's settings.py file. For example: # settings.py MIDDLEWARES = { 'myproject.middlewares.ExampleMiddleware' : 500 , } The number 500 represents the order in which the middleware will be executed. Lower numbers are processed first. What is a Scrapy Pipeline and How to Create One Scrapy pipeline is a mechanism that allows you to process the items scraped by the spider. It is used to perform various operations on the scraped data, such as cleaning, validation, and storing it in a database or file. Pipelines are defined in the pipelines.py file in your Scrapy project directory. To create a basic Scrapy pipeline, you need to define a Python class that implements a few methods. Here's a step-by-step guide: Open your Scrapy project directory and navigate to the pipelines.py file. Create a new Python class that inherits from the scrapy.ItemPipeline class. This class will define the operations that will be performed on the scraped data. Implement the init() method, which will be called when the pipeline is created. This method can be used to initialize any resources that the pipeline needs. Implement the process_item() method, which will be called for each item scraped by the spider. - This method should return the item, optionally modified or filtered. Optionally, implement the close_spider() method, which will be called when the spider finishes. This method can be used to clean up any resources used by the pipeline. Here's an example of a basic Scrapy pipeline: class MyPipeline ( object ): def __init__ ( self ): self . file = open ( \"data.txt\" , \"w\" ) def process_item ( self , item , spider ): line = json . dumps ( dict ( item )) + \" \\n \" self . file . write ( line ) return item def close_spider ( self , spider ): self . file . close () In this example, the pipeline writes the scraped data to a text file named \"data.txt\" and returns the original item. Note that the JSON serialization of the item is used to write it to the file. To enable the pipeline, you need to add its class to the ITEM_PIPELINES setting in the settings.py file: ITEM_PIPELINES = { 'myproject.pipelines.MyPipeline' : 300 , } In this example, the pipeline class is named MyPipeline and is located in the myproject.pipelines module. The number 300 specifies the order in which the pipeline will be executed relative to other pipelines (lower numbers are executed first).","title":"Introduction to Scrapy"},{"location":"tutorials/intro_to_scrapy/#introduction-to-scrapy","text":"In this tutorial, we will learn about Scrapy spiders and how they can be used for web scraping. We will cover the following topics: Introduction to Scrapy spiders Creating a simple spider Setting up a Scrapy project Using Scrapy.Item Configuring Scrapy middleware Creating a basic Scrapy pipeline","title":"Introduction to Scrapy"},{"location":"tutorials/intro_to_scrapy/#introduction-to-scrapy-spiders","text":"Scrapy spiders are classes that define how a specific site or group of sites should be scraped, including how to perform the crawl (i.e., follow links) and how to extract structured data from their pages (i.e., scraping items). Spiders are the main building blocks of a Scrapy project and are responsible for managing the process of gathering data from websites.","title":"Introduction to Scrapy Spiders"},{"location":"tutorials/intro_to_scrapy/#what-are-scrapy-spiders-used-for","text":"Scrapy spiders are used to: Crawl web pages and follow links to other pages. Extract and process structured data from web pages. Store the extracted data in the desired format, such as JSON, CSV, or XML.","title":"What are Scrapy spiders used for?"},{"location":"tutorials/intro_to_scrapy/#scrapy-spider","text":"scrapy.Spider is the base class for all Scrapy spiders. To create a simple spider, you need to subclass scrapy.Spider and define the following attributes and methods: name : A unique identifier for the spider. start_urls : A list of URLs where the spider will begin to crawl. parse() : A method that will be called to handle the response downloaded for each of the requests made. Here's an example of a simple spider: import scrapy class ExampleSpider ( scrapy . Spider ): name = \"example_spider\" start_urls = [ 'http://example.com' ] def parse ( self , response ): self . log ( 'Visited %s ' % response . url )","title":"Scrapy Spider"},{"location":"tutorials/intro_to_scrapy/#scrapy-project-setup-overview","text":"To set up a new Scrapy project, run the following command in your terminal: scrapy startproject myproject This will create a new directory named myproject with the following structure: myproject/ scrapy.cfg myproject/ __init__.py items.py middlewares.py pipelines.py settings.py spiders/ __init__.py The Scrapy project structure is a standardized layout for organizing your Scrapy spider code. The structure consists of a top-level project directory, which contains a configuration file and a subdirectory with the same name as the project. This subdirectory is where the majority of the code resides. Here's a breakdown of the files and directories in the Scrapy project structure: scrapy.cfg : This is the main configuration file for your Scrapy project. It tells Scrapy how to find your project code and where to store output files. myproject/ : This is the subdirectory with the same name as your project. It contains the bulk of your Scrapy code. init.py : This is an empty file that tells Python to treat the directory as a Python package. items.py : This file defines the data items that your spider will extract from websites. Each item corresponds to a specific piece of information that you want to scrape. middlewares.py: This file contains code for manipulating requests and responses as they pass through the Scrapy framework. This is useful for things like modifying headers or handling cookies. pipelines.py : This file defines the processing steps that your scraped items will go through. This can include things like cleaning and validating data or storing it in a database. settings.py: This file contains all of the settings for your Scrapy project, including things like user agents, download delays, and database connection information. spiders/ : This subdirectory is where you'll put the actual spider code. Each spider is defined in its own Python file in this directory. init.py : This is another empty file that tells Python to treat the directory as a Python package. Overall, this structure helps keep your Scrapy project organized and easy to navigate. By separating your code into logical modules, you can more easily maintain and extend your spiders as your scraping needs evolve.","title":"Scrapy Project Setup Overview"},{"location":"tutorials/intro_to_scrapy/#what-is-scrapyitem-and-how-to-create-one","text":"Scrapy items are simple Python classes used to define the structure of the data you want to scrape. They work like containers and allow you to define custom fields for the data you want to collect. Here's an example of how to create a Scrapy item: import scrapy class ExampleItem ( scrapy . Item ): title = scrapy . Field () url = scrapy . Field () description = scrapy . Field ()","title":"What is Scrapy.Item and How to Create One"},{"location":"tutorials/intro_to_scrapy/#what-is-scrapy-middleware-and-how-to-configure-it","text":"Scrapy middleware is a way to process the requests and responses that flow through the Scrapy engine. Middleware components can be used to modify the requests and responses or to perform additional processing on them. To configure a middleware in Scrapy, add it to the MIDDLEWARES setting in your project's settings.py file. For example: # settings.py MIDDLEWARES = { 'myproject.middlewares.ExampleMiddleware' : 500 , } The number 500 represents the order in which the middleware will be executed. Lower numbers are processed first.","title":"What is Scrapy Middleware and How to Configure It"},{"location":"tutorials/intro_to_scrapy/#what-is-a-scrapy-pipeline-and-how-to-create-one","text":"Scrapy pipeline is a mechanism that allows you to process the items scraped by the spider. It is used to perform various operations on the scraped data, such as cleaning, validation, and storing it in a database or file. Pipelines are defined in the pipelines.py file in your Scrapy project directory. To create a basic Scrapy pipeline, you need to define a Python class that implements a few methods. Here's a step-by-step guide: Open your Scrapy project directory and navigate to the pipelines.py file. Create a new Python class that inherits from the scrapy.ItemPipeline class. This class will define the operations that will be performed on the scraped data. Implement the init() method, which will be called when the pipeline is created. This method can be used to initialize any resources that the pipeline needs. Implement the process_item() method, which will be called for each item scraped by the spider. - This method should return the item, optionally modified or filtered. Optionally, implement the close_spider() method, which will be called when the spider finishes. This method can be used to clean up any resources used by the pipeline. Here's an example of a basic Scrapy pipeline: class MyPipeline ( object ): def __init__ ( self ): self . file = open ( \"data.txt\" , \"w\" ) def process_item ( self , item , spider ): line = json . dumps ( dict ( item )) + \" \\n \" self . file . write ( line ) return item def close_spider ( self , spider ): self . file . close () In this example, the pipeline writes the scraped data to a text file named \"data.txt\" and returns the original item. Note that the JSON serialization of the item is used to write it to the file. To enable the pipeline, you need to add its class to the ITEM_PIPELINES setting in the settings.py file: ITEM_PIPELINES = { 'myproject.pipelines.MyPipeline' : 300 , } In this example, the pipeline class is named MyPipeline and is located in the myproject.pipelines module. The number 300 specifies the order in which the pipeline will be executed relative to other pipelines (lower numbers are executed first).","title":"What is a Scrapy Pipeline and How to Create One"},{"location":"tutorials/scraping_myntra_product_data_part1/","text":"Scraping Myntra Product Data Part1: Product Page URLs In this tutorial, we'll be looking at how to scrape product page URLs from Myntra using Scrapy, which is a crucial step in building the ClosetCoach project. By collecting product information from Myntra's website, we can use it to train machine learning models and provide personalized fashion recommendations to users. Let's take a closer look at the source code provided in the previous question. First, we define the spider's start_requests method to initiate requests to the URLs provided as start_urls . These URLs can be updated to target specific categories of products that we want to collect information on. For example, we could target men's clothing with the following code: class ProductPageUrlScraper ( scrapy . Spider ): name = \"product_page_url_scraper\" allowed_domains = [ \"myntra.com\" ] start_urls = [ \"https://www.myntra.com/men-clothing\" ] Next, we define the parse method, which extracts product page URLs and product categories using ItemLoader and XPath selectors. We then yield a ProductPageUrl item object, which contains the URL and category of the product: def parse ( self , response : scrapy . http . Response ) -> scrapy . Item : loader = ItemLoader ( item = ProductPageUrl (), response = response ) loader . add_xpath ( \"urls\" , \"/html/body/div[2]/div/main/div[3]/div[2]/div/div[2]/section/ul/li/a/@href\" , ) loader . add_value ( \"category\" , response . url ) yield loader . load_item () Finally, we check if there's a next page using XPath, and if so, we initiate a new request for that page. We also keep track of the page number using a dictionary called PAGE_NUMS , which ensures that we don't scrape more pages than necessary: next_page = response . xpath ( '//*[@id=\"desktopSearchResults\"]/div[2]/section/div[2]/ul/li[12]/a/@href' ) . extract_first () if next_page is not None : if PAGE_NUMS [ response . url . split ( \"/\" )[ - 1 ] . split ( \"?\" )[ 0 ]] < MAX_PAGE_NUM : yield scrapy . Request ( url = next_page , callback = self . parse ) And that's it! By running this spider, we can collect product page URLs and categories from Myntra's website, which will be used in the ClosetCoach project to provide personalized fashion recommendations to users. I hope this tutorial has been helpful in getting started with Scrapy and web scraping. Happy coding!","title":"Scraping Myntra Product Data Part1"},{"location":"tutorials/scraping_myntra_product_data_part1/#scraping-myntra-product-data-part1-product-page-urls","text":"In this tutorial, we'll be looking at how to scrape product page URLs from Myntra using Scrapy, which is a crucial step in building the ClosetCoach project. By collecting product information from Myntra's website, we can use it to train machine learning models and provide personalized fashion recommendations to users. Let's take a closer look at the source code provided in the previous question. First, we define the spider's start_requests method to initiate requests to the URLs provided as start_urls . These URLs can be updated to target specific categories of products that we want to collect information on. For example, we could target men's clothing with the following code: class ProductPageUrlScraper ( scrapy . Spider ): name = \"product_page_url_scraper\" allowed_domains = [ \"myntra.com\" ] start_urls = [ \"https://www.myntra.com/men-clothing\" ] Next, we define the parse method, which extracts product page URLs and product categories using ItemLoader and XPath selectors. We then yield a ProductPageUrl item object, which contains the URL and category of the product: def parse ( self , response : scrapy . http . Response ) -> scrapy . Item : loader = ItemLoader ( item = ProductPageUrl (), response = response ) loader . add_xpath ( \"urls\" , \"/html/body/div[2]/div/main/div[3]/div[2]/div/div[2]/section/ul/li/a/@href\" , ) loader . add_value ( \"category\" , response . url ) yield loader . load_item () Finally, we check if there's a next page using XPath, and if so, we initiate a new request for that page. We also keep track of the page number using a dictionary called PAGE_NUMS , which ensures that we don't scrape more pages than necessary: next_page = response . xpath ( '//*[@id=\"desktopSearchResults\"]/div[2]/section/div[2]/ul/li[12]/a/@href' ) . extract_first () if next_page is not None : if PAGE_NUMS [ response . url . split ( \"/\" )[ - 1 ] . split ( \"?\" )[ 0 ]] < MAX_PAGE_NUM : yield scrapy . Request ( url = next_page , callback = self . parse ) And that's it! By running this spider, we can collect product page URLs and categories from Myntra's website, which will be used in the ClosetCoach project to provide personalized fashion recommendations to users. I hope this tutorial has been helpful in getting started with Scrapy and web scraping. Happy coding!","title":"Scraping Myntra Product Data Part1: Product Page URLs"},{"location":"tutorials/scraping_myntra_product_data_part2/","text":"Scraping Myntra Product Data Part2: Product Data and Metadata Welcome back! In our previous tutorial , we learned how to use Scrapy to collect product page URLs from Myntra. In this sequel tutorial, we will build on that knowledge and learn how to use Scrapy to scrape product data from those URLs. By collecting product data, we can build machine learning models and provide personalized fashion recommendations to users through ClosetCoach. This will help users save time and make more informed purchasing decisions The source code provided in the previous tutorial showed a spider called ProductDetailsScraperSpider . This spider extracts product data using ItemLoader and XPath selectors. The data includes the product name, description, rating, number of reviews, price after discount, category, and image URLs. Additionally, it extracts product metadata, such as specifications and other details. This spider is crucial for the ClosetCoach project, as it enables us to collect large amounts of product data from Myntra. With this data, we can train machine learning models to recommend products that match a user's style and preferences. This will help users to save time and make more informed purchasing decisions. Now that you understand the importance of this spider for the ClosetCoach project, let's take a closer look at the code. The start_requests method initiates requests to the URLs provided as start_urls. For each URL, it checks if the product page details already exist and, if not, initiates a request to scrape the data using scrapy.Request . def start_requests ( self ): urls = getattr ( self , \"start_urls\" , []) . split ( \",\" ) if isinstance ( urls , str ): urls = [ urls ] for url in urls : # check if product page details already exists url_exists = check_if_product_exists ( url ) if not url_exists : logging . info ( f \"Scraping product page details from { url } \" ) yield scrapy . Request ( url = url , callback = self . parse ) else : logging . info ( f \"Product page details already exists for { url } \" ) The parse method extracts the data from the response and creates a ProductDetails object using ItemLoader . The ItemLoader object allows us to load data into ProductDetails fields using XPath selectors. def parse ( self , response ): # generate selenium's response html = get_page_source ( response . url , click_button_xpath = '//*[@id=\"mountRoot\"]/div/div[1]/main/div[2]/div[2]/div[3]/div/div[4]/div[2]' ) # convert selenium's resonse as scrapy based html response html_response = HtmlResponse ( url = response . url , body = html , encoding = \"utf-8\" ) loader = ItemLoader ( item = ProductDetails (), response = html_response ) # add data to item using xpath selectors loader . add_value ( \"product_page_url\" , html_response . url ) loader . add_value ( \"product_id\" , html_response . url . split ( \"/\" )[ - 2 ]) loader . add_xpath ( \"product_name\" , '//*[@id=\"mountRoot\"]/div/div[1]/main/div[2]/div[2]/div[1]/h1[1]/text()' ) loader . add_xpath ( \"product_description\" , '//*[@id=\"mountRoot\"]/div/div[1]/main/div[2]/div[2]/div[1]/h1[2]/text()' ) loader . add_xpath ( \"product_rating\" , '//*[@id=\"mountRoot\"]/div/div[1]/main/div[2]/div[2]/div[1]/div/div/div/div[1]/text()' ) loader . add_xpath ( \"product_num_reviews\" , '//*[@id=\"mountRoot\"]/div/div[1]/main/div[2]/div[2]/div[1]/div/div/div/div[3]/text()' ) loader . add_xpath ( \"product_price_after_discount\" , '//*[@id=\"mountRoot\"]/div/div[1]/main/div[2]/div[2]/div[1]/div/p[1]/span[1]/strong/text()' ) Next lets see the section that extracts image URLs divs = html_response . xpath ( '//div[@class=\"image-grid-image\"]' ) for div in divs : # Extract the style attribute value style = div . xpath ( \"@style\" ) . get () # Use a regular expression to extract the URL from the style attribute value url_match = re . search ( r 'url\\([\" \\' ]?(.+?)[\" \\' ]?\\)' , style ) if url_match : url = url_match . group ( 1 ) loader . add_value ( \"product_image_urls\" , url ) This code extracts the image URLs for a product from the html_response . It first selects all div elements with a class of \"image-grid-image\" using XPath selectors. It then loops over each div element and extracts the value of the style attribute using xpath(\"@style\").get() . It then uses a regular expression to extract the URL from the style attribute value using re.search . Finally, it adds each URL to the product_image_urls field of the ProductDetails object using loader.add_value . The section that extracts product metadata is shown below: product_details_div = html_response . xpath ( '//div[@class=\"pdp-productDescriptorsContainer\"]' ) headings = product_details_div . xpath ( \".//h4/text()\" ) . getall () contents = product_details_div . xpath ( \".//p/text()\" ) . getall () details_dict = {} for i in range ( len ( headings )): try : details_dict [ headings [ i ] . strip ()] = contents [ i ] . strip () except IndexError : pass spec_rows = product_details_div . xpath ( './/div[contains(@class, \"index-row\")]' ) for row in spec_rows : key = row . xpath ( './/div[contains(@class, \"index-rowKey\")]/text()' ) . get () value = row . xpath ( './/div[contains(@class, \"index-rowValue\")]/text()' ) . get () if key and value : details_dict [ key . strip ()] = value . strip () loader . add_value ( \"product_metadata\" , details_dict ) This code extracts the product metadata, such as specifications and other details, from the html_response . It first selects the div element with a class of \"pdp-productDescriptorsContainer\" using XPath selectors. It then selects all h4 and p elements within this div using .//h4/text() and .//p/text() XPath selectors, respectively. It creates an empty dictionary called details_dict and populates it with the product metadata by iterating over the headings and contents lists. For each iteration, it tries to add the heading as a key and the corresponding content as a value to details_dict . Next, it selects all div elements containing \"index-row\" in their class using './/div[contains(@class, \"index-row\")]' XPath selector. It then extracts the key and value of each row using './/div[contains(@class, \"index-rowKey\")]/text()' and './/div[contains(@class, \"index-rowValue\")]/text()' XPath selectors, respectively. Finally, it adds the key-value pairs to details_dict . Finally, it adds the entire details_dict to the product_metadata field of the ProductDetails object using loader.add_value .","title":"Scraping Myntra Product Data Part2"},{"location":"tutorials/scraping_myntra_product_data_part2/#scraping-myntra-product-data-part2-product-data-and-metadata","text":"Welcome back! In our previous tutorial , we learned how to use Scrapy to collect product page URLs from Myntra. In this sequel tutorial, we will build on that knowledge and learn how to use Scrapy to scrape product data from those URLs. By collecting product data, we can build machine learning models and provide personalized fashion recommendations to users through ClosetCoach. This will help users save time and make more informed purchasing decisions The source code provided in the previous tutorial showed a spider called ProductDetailsScraperSpider . This spider extracts product data using ItemLoader and XPath selectors. The data includes the product name, description, rating, number of reviews, price after discount, category, and image URLs. Additionally, it extracts product metadata, such as specifications and other details. This spider is crucial for the ClosetCoach project, as it enables us to collect large amounts of product data from Myntra. With this data, we can train machine learning models to recommend products that match a user's style and preferences. This will help users to save time and make more informed purchasing decisions. Now that you understand the importance of this spider for the ClosetCoach project, let's take a closer look at the code. The start_requests method initiates requests to the URLs provided as start_urls. For each URL, it checks if the product page details already exist and, if not, initiates a request to scrape the data using scrapy.Request . def start_requests ( self ): urls = getattr ( self , \"start_urls\" , []) . split ( \",\" ) if isinstance ( urls , str ): urls = [ urls ] for url in urls : # check if product page details already exists url_exists = check_if_product_exists ( url ) if not url_exists : logging . info ( f \"Scraping product page details from { url } \" ) yield scrapy . Request ( url = url , callback = self . parse ) else : logging . info ( f \"Product page details already exists for { url } \" ) The parse method extracts the data from the response and creates a ProductDetails object using ItemLoader . The ItemLoader object allows us to load data into ProductDetails fields using XPath selectors. def parse ( self , response ): # generate selenium's response html = get_page_source ( response . url , click_button_xpath = '//*[@id=\"mountRoot\"]/div/div[1]/main/div[2]/div[2]/div[3]/div/div[4]/div[2]' ) # convert selenium's resonse as scrapy based html response html_response = HtmlResponse ( url = response . url , body = html , encoding = \"utf-8\" ) loader = ItemLoader ( item = ProductDetails (), response = html_response ) # add data to item using xpath selectors loader . add_value ( \"product_page_url\" , html_response . url ) loader . add_value ( \"product_id\" , html_response . url . split ( \"/\" )[ - 2 ]) loader . add_xpath ( \"product_name\" , '//*[@id=\"mountRoot\"]/div/div[1]/main/div[2]/div[2]/div[1]/h1[1]/text()' ) loader . add_xpath ( \"product_description\" , '//*[@id=\"mountRoot\"]/div/div[1]/main/div[2]/div[2]/div[1]/h1[2]/text()' ) loader . add_xpath ( \"product_rating\" , '//*[@id=\"mountRoot\"]/div/div[1]/main/div[2]/div[2]/div[1]/div/div/div/div[1]/text()' ) loader . add_xpath ( \"product_num_reviews\" , '//*[@id=\"mountRoot\"]/div/div[1]/main/div[2]/div[2]/div[1]/div/div/div/div[3]/text()' ) loader . add_xpath ( \"product_price_after_discount\" , '//*[@id=\"mountRoot\"]/div/div[1]/main/div[2]/div[2]/div[1]/div/p[1]/span[1]/strong/text()' ) Next lets see the section that extracts image URLs divs = html_response . xpath ( '//div[@class=\"image-grid-image\"]' ) for div in divs : # Extract the style attribute value style = div . xpath ( \"@style\" ) . get () # Use a regular expression to extract the URL from the style attribute value url_match = re . search ( r 'url\\([\" \\' ]?(.+?)[\" \\' ]?\\)' , style ) if url_match : url = url_match . group ( 1 ) loader . add_value ( \"product_image_urls\" , url ) This code extracts the image URLs for a product from the html_response . It first selects all div elements with a class of \"image-grid-image\" using XPath selectors. It then loops over each div element and extracts the value of the style attribute using xpath(\"@style\").get() . It then uses a regular expression to extract the URL from the style attribute value using re.search . Finally, it adds each URL to the product_image_urls field of the ProductDetails object using loader.add_value . The section that extracts product metadata is shown below: product_details_div = html_response . xpath ( '//div[@class=\"pdp-productDescriptorsContainer\"]' ) headings = product_details_div . xpath ( \".//h4/text()\" ) . getall () contents = product_details_div . xpath ( \".//p/text()\" ) . getall () details_dict = {} for i in range ( len ( headings )): try : details_dict [ headings [ i ] . strip ()] = contents [ i ] . strip () except IndexError : pass spec_rows = product_details_div . xpath ( './/div[contains(@class, \"index-row\")]' ) for row in spec_rows : key = row . xpath ( './/div[contains(@class, \"index-rowKey\")]/text()' ) . get () value = row . xpath ( './/div[contains(@class, \"index-rowValue\")]/text()' ) . get () if key and value : details_dict [ key . strip ()] = value . strip () loader . add_value ( \"product_metadata\" , details_dict ) This code extracts the product metadata, such as specifications and other details, from the html_response . It first selects the div element with a class of \"pdp-productDescriptorsContainer\" using XPath selectors. It then selects all h4 and p elements within this div using .//h4/text() and .//p/text() XPath selectors, respectively. It creates an empty dictionary called details_dict and populates it with the product metadata by iterating over the headings and contents lists. For each iteration, it tries to add the heading as a key and the corresponding content as a value to details_dict . Next, it selects all div elements containing \"index-row\" in their class using './/div[contains(@class, \"index-row\")]' XPath selector. It then extracts the key and value of each row using './/div[contains(@class, \"index-rowKey\")]/text()' and './/div[contains(@class, \"index-rowValue\")]/text()' XPath selectors, respectively. Finally, it adds the key-value pairs to details_dict . Finally, it adds the entire details_dict to the product_metadata field of the ProductDetails object using loader.add_value .","title":"Scraping Myntra Product Data Part2: Product Data and Metadata"}]}
